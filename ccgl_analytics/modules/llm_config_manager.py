"""
å¤§è¯­è¨€æ¨¡å‹é…ç½®ç®¡ç†æ¨¡å—

æ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹çš„é…ç½®ç®¡ç†å’Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢æ¥å£ã€‚
"""

import os
import logging
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
import json

# Optional imports for different AI providers
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

class LLMConfigManager:
    """å¤§è¯­è¨€æ¨¡å‹é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–LLMé…ç½®ç®¡ç†å™¨
        
        Args:
            config: LLMé…ç½®ä¿¡æ¯
        """
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        self.clients = {}
        self.available_models = {}
        self._initialize_clients()
    
    def _initialize_clients(self):
        """åˆå§‹åŒ–å„ç§LLMå®¢æˆ·ç«¯"""
        # OpenAI GPTå®¢æˆ·ç«¯
        if OPENAI_AVAILABLE:
            openai_key = self.config.get('openai_api_key') or os.getenv('OPENAI_API_KEY')
            if openai_key:
                try:
                    self.clients['openai'] = openai.OpenAI(api_key=openai_key)
                    self.available_models['openai'] = [
                        'gpt-4o', 'gpt-4', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k'
                    ]
                    self.logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            else:
                self.logger.warning("æœªé…ç½®OpenAI APIå¯†é’¥")
        
        # Anthropic Claudeå®¢æˆ·ç«¯
        if ANTHROPIC_AVAILABLE:
            anthropic_key = self.config.get('anthropic_api_key') or os.getenv('ANTHROPIC_API_KEY')
            if anthropic_key:
                try:
                    self.clients['anthropic'] = anthropic.Anthropic(api_key=anthropic_key)
                    self.available_models['anthropic'] = [
                        'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307'
                    ]
                    self.logger.info("Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            else:
                self.logger.warning("æœªé…ç½®Anthropic APIå¯†é’¥")
        
        # é€šä¹‰åƒé—®å’Œæ™ºè°±GLMç­‰å…¶ä»–æ¨¡å‹çš„é…ç½®
        self._setup_other_models()
        
        if not self.clients:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    
    def _setup_other_models(self):
        """è®¾ç½®å…¶ä»–æ¨¡å‹é…ç½®"""
        # æ™ºè°±GLMé…ç½®
        zhipu_key = self.config.get('zhipu_api_key') or os.getenv('ZHIPU_API_KEY')
        if zhipu_key:
            self.available_models['zhipu'] = ['glm-4', 'glm-3-turbo']
            self.logger.info("æ™ºè°±GLMé…ç½®å·²è®¾ç½®")
        
        # é€šä¹‰åƒé—®é…ç½®
        qwen_key = self.config.get('qwen_api_key') or os.getenv('QWEN_API_KEY')
        if qwen_key:
            self.available_models['qwen'] = ['qwen-turbo', 'qwen-max', 'qwen-max-1201']
            self.logger.info("é€šä¹‰åƒé—®é…ç½®å·²è®¾ç½®")
    
    def get_available_models(self) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return self.available_models.copy()
    
    def natural_language_query(self, query: str, data_context: str = "", 
                             model: str = None, max_tokens: int = 2000) -> Dict[str, Any]:
        """
        è‡ªç„¶è¯­è¨€æ•°æ®æŸ¥è¯¢æ¥å£
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            data_context: æ•°æ®ä¸Šä¸‹æ–‡ä¿¡æ¯
            model: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            # é€‰æ‹©æ¨¡å‹
            if model is None:
                model = self._select_best_model()
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_analysis_prompt(query, data_context)
            
            # æ‰§è¡ŒæŸ¥è¯¢
            if model.startswith('gpt') and 'openai' in self.clients:
                response = self._query_openai(prompt, model, max_tokens)
            elif model.startswith('claude') and 'anthropic' in self.clients:
                response = self._query_anthropic(prompt, model, max_tokens)
            else:
                # æ¨¡æ‹Ÿå“åº”
                response = self._simulate_response(query, data_context)
            
            return {
                'success': True,
                'query': query,
                'model_used': model,
                'response': response,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"è‡ªç„¶è¯­è¨€æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'query': query,
                'timestamp': datetime.now().isoformat()
            }
    
    def _select_best_model(self) -> str:
        """é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹"""
        # ä¼˜å…ˆçº§æ’åº
        priority_models = [
            'gpt-4o', 'gpt-4', 'claude-3-opus-20240229', 
            'claude-3-sonnet-20240229', 'gpt-3.5-turbo'
        ]
        
        for model in priority_models:
            for provider, models in self.available_models.items():
                if model in models and provider in self.clients:
                    return model
        
        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        for provider, models in self.available_models.items():
            if provider in self.clients and models:
                return models[0]
        
        return 'simulated'
    
    def _build_analysis_prompt(self, query: str, data_context: str) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»“å‚¨ç®¡ç†æ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ•°æ®ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

æ•°æ®ä¸Šä¸‹æ–‡ï¼š
{data_context}

ç”¨æˆ·é—®é¢˜ï¼š
{query}

è¯·æä¾›ï¼š
1. å¯¹é—®é¢˜çš„ç†è§£å’Œåˆ†æ
2. åŸºäºæ•°æ®çš„è§è§£å’Œå‘ç°
3. å…·ä½“çš„æ•°æ®æ”¯æŒ
4. å®ç”¨çš„å»ºè®®å’Œç»“è®º

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ã€‚"""
        
        return prompt
    
    def _query_openai(self, prompt: str, model: str, max_tokens: int) -> str:
        """ä½¿ç”¨OpenAI APIæŸ¥è¯¢"""
        try:
            client = self.clients['openai']
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"OpenAIæŸ¥è¯¢å¤±è´¥: {e}")
    
    def _query_anthropic(self, prompt: str, model: str, max_tokens: int) -> str:
        """ä½¿ç”¨Anthropic APIæŸ¥è¯¢"""
        try:
            client = self.clients['anthropic']
            response = client.messages.create(
                model=model,
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        except Exception as e:
            raise Exception(f"AnthropicæŸ¥è¯¢å¤±è´¥: {e}")
    
    def _simulate_response(self, query: str, data_context: str) -> str:
        """æ¨¡æ‹ŸAIå“åº”ï¼ˆå½“æ²¡æœ‰å¯ç”¨çš„APIæ—¶ï¼‰"""
        return f"""åŸºäºæ‚¨çš„æŸ¥è¯¢"{query}"ï¼Œæˆ‘æä¾›ä»¥ä¸‹åˆ†æï¼š

ğŸ” æŸ¥è¯¢ç†è§£ï¼š
æ‚¨è¯¢é—®äº†å…³äºä»“å‚¨ç®¡ç†æ•°æ®çš„é—®é¢˜ã€‚åŸºäºå½“å‰çš„æ•°æ®ä¸Šä¸‹æ–‡ï¼Œæˆ‘å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€ä¸ªæ¶‰åŠä»“åº“è¿è¥çš„å¤æ‚æ•°æ®é›†ã€‚

ğŸ“Š æ•°æ®åˆ†æè§è§£ï¼š
1. æ•°æ®å®Œæ•´æ€§ï¼šå½“å‰æ•°æ®é›†æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ•°æ®è´¨é‡
2. ä¸šåŠ¡æ¨¡å¼ï¼šå¯ä»¥è¯†åˆ«å‡ºæ˜æ˜¾çš„è¿è¥æ¨¡å¼å’Œè¶‹åŠ¿
3. å¼‚å¸¸æƒ…å†µï¼šæ£€æµ‹åˆ°ä¸€äº›éœ€è¦å…³æ³¨çš„å¼‚å¸¸æ•°æ®ç‚¹

ğŸ’¡ å»ºè®®ï¼š
1. å»ºè®®è¿›ä¸€æ­¥å…³æ³¨æ•°æ®è´¨é‡ç®¡ç†
2. å¯ä»¥è€ƒè™‘å®æ–½é¢„æµ‹æ€§åˆ†æ
3. ä¼˜åŒ–åº“å­˜ç®¡ç†ç­–ç•¥

âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿå“åº”ï¼Œå»ºè®®é…ç½®çœŸå®çš„AIæ¨¡å‹APIä»¥è·å¾—æ›´å‡†ç¡®çš„åˆ†æã€‚

å¦‚éœ€æ›´è¯¦ç»†çš„åˆ†æï¼Œè¯·é…ç½®OpenAIæˆ–Anthropic APIå¯†é’¥ã€‚"""
    
    def generate_business_insights(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ™ºèƒ½ä¸šåŠ¡æ´å¯Ÿ
        
        Args:
            analysis_results: åˆ†æç»“æœæ•°æ®
            
        Returns:
            ä¸šåŠ¡æ´å¯ŸæŠ¥å‘Š
        """
        try:
            # æå–å…³é”®æŒ‡æ ‡
            key_metrics = self._extract_key_metrics(analysis_results)
            
            # æ„å»ºæ´å¯Ÿæç¤º
            insights_prompt = self._build_insights_prompt(key_metrics)
            
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹ç”Ÿæˆæ´å¯Ÿ
            model = self._select_best_model()
            
            if model != 'simulated':
                insights = self.natural_language_query(
                    "è¯·åŸºäºè¿™äº›æ•°æ®ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿå’Œå»ºè®®", 
                    insights_prompt
                )
            else:
                insights = self._generate_simulated_insights(key_metrics)
            
            return {
                'success': True,
                'insights': insights,
                'key_metrics': key_metrics,
                'generation_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'generation_time': datetime.now().isoformat()
            }
    
    def _extract_key_metrics(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å…³é”®æŒ‡æ ‡"""
        metrics = {}
        
        # æ•°æ®è´¨é‡æŒ‡æ ‡
        if 'data_quality_assessment' in analysis_results:
            quality = analysis_results['data_quality_assessment']
            metrics['data_quality'] = {
                'completeness': quality.get('data_quality_metrics', {}).get('completeness'),
                'total_rows': quality.get('basic_stats', {}).get('total_rows'),
                'missing_values': quality.get('missing_values', {}).get('total_missing')
            }
        
        # èšç±»åˆ†æç»“æœ
        if 'comprehensive_analysis' in analysis_results:
            comp_analysis = analysis_results['comprehensive_analysis']
            if 'clustering_analysis' in comp_analysis:
                clustering = comp_analysis['clustering_analysis']
                metrics['clustering'] = {
                    'optimal_clusters': clustering.get('optimal_clusters'),
                    'evaluation_metrics': clustering.get('evaluation_metrics')
                }
            
            # å¼‚å¸¸æ£€æµ‹ç»“æœ
            if 'anomaly_detection' in comp_analysis:
                anomaly = comp_analysis['anomaly_detection']
                metrics['anomalies'] = anomaly.get('summary', {})
        
        return metrics
    
    def _build_insights_prompt(self, key_metrics: Dict[str, Any]) -> str:
        """æ„å»ºæ´å¯Ÿç”Ÿæˆæç¤º"""
        return f"""åŸºäºä»¥ä¸‹ä»“å‚¨ç®¡ç†æ•°æ®åˆ†æçš„å…³é”®æŒ‡æ ‡ï¼š

{json.dumps(key_metrics, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆä¸“ä¸šçš„ä¸šåŠ¡æ´å¯Ÿï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®è´¨é‡è¯„ä¼°
2. è¿è¥æ•ˆç‡åˆ†æ  
3. é£é™©è¯†åˆ«
4. ä¼˜åŒ–å»ºè®®
5. è¶‹åŠ¿é¢„æµ‹"""
    
    def _generate_simulated_insights(self, key_metrics: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„ä¸šåŠ¡æ´å¯Ÿ"""
        return {
            'response': f"""ğŸ“ˆ CCGLä»“å‚¨ç®¡ç†ä¸šåŠ¡æ´å¯ŸæŠ¥å‘Š

ğŸ¯ æ•°æ®è´¨é‡è¯„ä¼°ï¼š
â€¢ æ•°æ®å®Œæ•´æ€§è¾¾åˆ° {key_metrics.get('data_quality', {}).get('completeness', 0.95)*100:.1f}%ï¼Œè¡¨ç°è‰¯å¥½
â€¢ æ€»è®¡ {key_metrics.get('data_quality', {}).get('total_rows', 0)} æ¡è®°å½•ï¼Œæ•°æ®è§„æ¨¡é€‚ä¸­
â€¢ ç¼ºå¤±å€¼ {key_metrics.get('data_quality', {}).get('missing_values', 0)} ä¸ªï¼Œéœ€è¦å…³æ³¨

ğŸ” èšç±»åˆ†ææ´å¯Ÿï¼š
â€¢ å‘ç° {key_metrics.get('clustering', {}).get('optimal_clusters', 3)} ä¸ªä¸»è¦å®¢æˆ·/äº§å“ç¾¤ä½“
â€¢ å»ºè®®é’ˆå¯¹ä¸åŒç¾¤ä½“åˆ¶å®šå·®å¼‚åŒ–ç­–ç•¥
â€¢ å¯ä¼˜åŒ–åº“å­˜é…ç½®å’ŒæœåŠ¡ç­–ç•¥

âš ï¸ é£é™©è¯†åˆ«ï¼š
â€¢ æ£€æµ‹åˆ° {key_metrics.get('anomalies', {}).get('consensus_anomalies', 0)} ä¸ªå¼‚å¸¸æ•°æ®ç‚¹
â€¢ å»ºè®®å»ºç«‹å®æ—¶ç›‘æ§æœºåˆ¶
â€¢ éœ€è¦åŠ å¼ºæ•°æ®è´¨é‡æ§åˆ¶

ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼š
1. å®æ–½åŠ¨æ€åº“å­˜ç®¡ç†
2. å»ºç«‹é¢„æµ‹æ€§ç»´æŠ¤
3. ä¼˜åŒ–ä¾›åº”é“¾åè°ƒ
4. åŠ å¼ºæ•°æ®æ²»ç†

âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯åŸºäºæ¨¡æ‹ŸAIçš„åˆ†æï¼Œå»ºè®®é…ç½®çœŸå®AIæ¨¡å‹ä»¥è·å¾—æ›´æ·±å…¥çš„æ´å¯Ÿã€‚"""
        }
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯å†å²"""
        # è¿™é‡Œå¯ä»¥å®ç°å¯¹è¯å†å²çš„å­˜å‚¨å’Œæ£€ç´¢
        return []
    
    def clear_conversation_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        pass
    
    def get_config_summary(self) -> Dict[str, Any]:
        """è·å–é…ç½®æ‘˜è¦"""
        return {
            'available_providers': list(self.clients.keys()),
            'available_models': self.available_models,
            'default_model': self._select_best_model(),
            'openai_available': OPENAI_AVAILABLE and 'openai' in self.clients,
            'anthropic_available': ANTHROPIC_AVAILABLE and 'anthropic' in self.clients
        }