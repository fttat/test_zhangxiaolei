#!/usr/bin/env python3
"""
CCGLä»“å‚¨ç®¡ç†ç³»ç»Ÿæ•°æ®åˆ†æå·¥ç¨‹ - ä¸»ç¨‹åºï¼ˆåŸºç¡€åˆ†ææ¨¡å¼ï¼‰

æä¾›æ ‡å‡†çš„æ•°æ®è¿æ¥ã€è´¨é‡è¯„ä¼°å’Œæœºå™¨å­¦ä¹ åˆ†æåŠŸèƒ½ã€‚
"""

import os
import sys
import logging
import pandas as pd
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from ccgl_analytics.modules.data_connection import DataConnectionManager
from ccgl_analytics.modules.data_preprocessing import DataPreprocessor  
from ccgl_analytics.modules.analysis_core import AnalysisCore
from ccgl_analytics.utils.config_loader import ConfigLoader
from ccgl_analytics.utils.logger_setup import setup_logger

class CCGLAnalyticsMain:
    """CCGLåˆ†æç³»ç»Ÿä¸»ç¨‹åº"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¸»ç¨‹åº"""
        self.logger = setup_logger("ccgl_main")
        self.config_loader = ConfigLoader()
        self.db_manager = None
        self.preprocessor = None
        self.analyzer = None
        
    def initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        try:
            self.logger.info("=== CCGLä»“å‚¨ç®¡ç†ç³»ç»Ÿæ•°æ®åˆ†æå·¥ç¨‹å¯åŠ¨ ===")
            self.logger.info("æ¨¡å¼: åŸºç¡€åˆ†ææ¨¡å¼")
            
            # åŠ è½½é…ç½®
            config = self.config_loader.load_config()
            self.logger.info("é…ç½®åŠ è½½å®Œæˆ")
            
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ç®¡ç†å™¨
            db_config = config.get('database', {})
            if not db_config:
                # ä½¿ç”¨ç¤ºä¾‹é…ç½®
                db_config = {
                    'host': 'localhost',
                    'port': 3306,
                    'database': 'ccgl_warehouse',
                    'user': 'root',
                    'password': 'password'
                }
                self.logger.warning("ä½¿ç”¨é»˜è®¤æ•°æ®åº“é…ç½®ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
            
            # æ³¨æ„ï¼šå®é™…éƒ¨ç½²æ—¶éœ€è¦çœŸå®çš„æ•°æ®åº“è¿æ¥
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º
            self.db_manager = DataConnectionManager(db_config)
            self.logger.info("æ•°æ®åº“è¿æ¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
            preprocessing_config = config.get('preprocessing', {})
            self.preprocessor = DataPreprocessor(preprocessing_config)
            self.logger.info("æ•°æ®é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–åˆ†ææ ¸å¿ƒ
            analysis_config = config.get('analysis', {})
            self.analyzer = AnalysisCore(analysis_config)
            self.logger.info("åˆ†ææ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ")
            
            self.logger.info("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def generate_sample_data(self) -> pd.DataFrame:
        """ç”Ÿæˆç¤ºä¾‹ä»“å‚¨æ•°æ®ç”¨äºæ¼”ç¤º"""
        try:
            import numpy as np
            
            # ç”Ÿæˆç¤ºä¾‹ä»“å‚¨ç®¡ç†æ•°æ®
            n_samples = 1000
            np.random.seed(42)
            
            # ä»“åº“ä¿¡æ¯
            warehouses = ['ä»“åº“A', 'ä»“åº“B', 'ä»“åº“C', 'ä»“åº“D']
            products = ['äº§å“001', 'äº§å“002', 'äº§å“003', 'äº§å“004', 'äº§å“005']
            suppliers = ['ä¾›åº”å•†ç”²', 'ä¾›åº”å•†ä¹™', 'ä¾›åº”å•†ä¸™']
            
            data = {
                'warehouse_id': np.random.choice(warehouses, n_samples),
                'product_id': np.random.choice(products, n_samples),
                'supplier_id': np.random.choice(suppliers, n_samples),
                'quantity': np.random.randint(1, 1000, n_samples),
                'unit_price': np.random.uniform(10, 500, n_samples),
                'storage_cost': np.random.uniform(1, 50, n_samples),
                'order_date': pd.date_range(start='2024-01-01', periods=n_samples, freq='H'),
                'delivery_days': np.random.randint(1, 30, n_samples),
                'quality_score': np.random.uniform(0.6, 1.0, n_samples),
                'temperature': np.random.uniform(15, 25, n_samples),
                'humidity': np.random.uniform(40, 80, n_samples)
            }
            
            # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼æ¥æ¨¡æ‹ŸçœŸå®æ•°æ®
            df = pd.DataFrame(data)
            
            # å¼•å…¥ç¼ºå¤±å€¼
            missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)
            df.loc[missing_indices[:len(missing_indices)//2], 'quality_score'] = np.nan
            df.loc[missing_indices[len(missing_indices)//2:], 'storage_cost'] = np.nan
            
            # å¼•å…¥å¼‚å¸¸å€¼
            outlier_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)
            df.loc[outlier_indices, 'unit_price'] = df.loc[outlier_indices, 'unit_price'] * 10
            
            self.logger.info(f"ç”Ÿæˆç¤ºä¾‹æ•°æ®: {df.shape}")
            return df
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç¤ºä¾‹æ•°æ®å¤±è´¥: {e}")
            raise
    
    def run_data_quality_assessment(self, df: pd.DataFrame) -> dict:
        """è¿è¡Œæ•°æ®è´¨é‡è¯„ä¼°"""
        try:
            self.logger.info("å¼€å§‹æ•°æ®è´¨é‡è¯„ä¼°")
            
            # æ¨¡æ‹Ÿè¡¨æ•°æ®è´¨é‡è¯„ä¼°
            quality_report = {
                'table_name': 'sample_warehouse_data',
                'assessment_time': datetime.now().isoformat(),
                'basic_stats': {
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
                    'numeric_columns': df.select_dtypes(include=['number']).shape[1],
                    'categorical_columns': df.select_dtypes(include=['object']).shape[1],
                    'datetime_columns': df.select_dtypes(include=['datetime64']).shape[1]
                },
                'data_quality_metrics': {
                    'completeness': (df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]),
                    'consistency': 0.95,  # æ¨¡æ‹Ÿå€¼
                    'validity': 0.98,     # æ¨¡æ‹Ÿå€¼
                    'uniqueness': len(df.drop_duplicates()) / len(df)
                },
                'missing_values': {
                    'total_missing': int(df.isnull().sum().sum()),
                    'missing_by_column': df.isnull().sum().to_dict(),
                    'missing_percentage_by_column': (df.isnull().sum() / len(df) * 100).to_dict()
                }
            }
            
            self.logger.info("æ•°æ®è´¨é‡è¯„ä¼°å®Œæˆ")
            return quality_report
            
        except Exception as e:
            self.logger.error(f"æ•°æ®è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            raise
    
    def run_comprehensive_analysis(self, df: pd.DataFrame) -> dict:
        """è¿è¡Œç»¼åˆæ•°æ®åˆ†æ"""
        try:
            self.logger.info("å¼€å§‹ç»¼åˆæ•°æ®åˆ†æ")
            
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("æ­¥éª¤1: æ•°æ®é¢„å¤„ç†")
            cleaned_data, preprocessing_report = self.preprocessor.clean_data(
                df, 
                missing_strategy='auto',
                outlier_method='iqr',
                scaling_method='standard'
            )
            
            # 2. èšç±»åˆ†æ
            self.logger.info("æ­¥éª¤2: èšç±»åˆ†æ")
            clustering_results = self.analyzer.clustering_analysis(
                cleaned_data,
                methods=['kmeans', 'dbscan', 'hierarchical'],
                n_clusters=None  # è‡ªåŠ¨ç¡®å®š
            )
            
            # 3. å¼‚å¸¸æ£€æµ‹
            self.logger.info("æ­¥éª¤3: å¼‚å¸¸æ£€æµ‹åˆ†æ")
            anomaly_results = self.analyzer.anomaly_detection(
                cleaned_data,
                methods=['isolation_forest', 'one_class_svm']
            )
            
            # 4. é™ç»´åˆ†æ
            self.logger.info("æ­¥éª¤4: é™ç»´åˆ†æ")
            reduction_results = self.analyzer.dimensionality_reduction(
                cleaned_data,
                methods=['pca', 'tsne'],
                n_components=2
            )
            
            # 5. ç‰¹å¾å·¥ç¨‹
            self.logger.info("æ­¥éª¤5: ç‰¹å¾å·¥ç¨‹")
            enhanced_data, feature_report = self.preprocessor.create_features(cleaned_data)
            
            # 6. å…³è”è§„åˆ™æŒ–æ˜ï¼ˆå¦‚æœæ•°æ®é€‚åˆï¼‰
            self.logger.info("æ­¥éª¤6: å…³è”è§„åˆ™æŒ–æ˜")
            try:
                rules_results = self.analyzer.association_rules_mining(
                    df[['warehouse_id', 'product_id', 'supplier_id']],
                    min_support=0.01,
                    min_confidence=0.5
                )
            except Exception as e:
                rules_results = {'error': str(e)}
                self.logger.warning(f"å…³è”è§„åˆ™æŒ–æ˜è·³è¿‡: {e}")
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = {
                'analysis_time': datetime.now().isoformat(),
                'data_preprocessing': preprocessing_report,
                'feature_engineering': feature_report,
                'clustering_analysis': clustering_results,
                'anomaly_detection': anomaly_results,
                'dimensionality_reduction': reduction_results,
                'association_rules': rules_results,
                'summary': self._generate_analysis_summary(
                    preprocessing_report, clustering_results, anomaly_results, reduction_results
                )
            }
            
            self.logger.info("ç»¼åˆæ•°æ®åˆ†æå®Œæˆ")
            return comprehensive_report
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆæ•°æ®åˆ†æå¤±è´¥: {e}")
            raise
    
    def _generate_analysis_summary(self, preprocessing_report, clustering_results, 
                                 anomaly_results, reduction_results) -> dict:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        try:
            summary = {
                'data_processing': {
                    'original_shape': preprocessing_report.get('original_shape'),
                    'final_shape': preprocessing_report.get('final_shape'),
                    'rows_removed': preprocessing_report.get('removed_rows', 0),
                    'processing_steps': len(preprocessing_report.get('processing_steps', []))
                },
                'clustering': {
                    'optimal_clusters': clustering_results.get('optimal_clusters'),
                    'best_method': None,
                    'best_silhouette_score': None
                },
                'anomalies': {
                    'consensus_anomalies': anomaly_results.get('summary', {}).get('common_anomaly_count', 0),
                    'total_samples': anomaly_results.get('summary', {}).get('total_samples', 0)
                },
                'dimensionality_reduction': {
                    'original_dimensions': reduction_results.get('original_dimensions'),
                    'variance_explained': None
                }
            }
            
            # æ‰¾æœ€ä½³èšç±»æ–¹æ³•
            if 'evaluation_metrics' in clustering_results:
                best_score = -1
                best_method = None
                for method, metrics in clustering_results['evaluation_metrics'].items():
                    if 'silhouette_score' in metrics and metrics['silhouette_score'] > best_score:
                        best_score = metrics['silhouette_score']
                        best_method = method
                
                summary['clustering']['best_method'] = best_method
                summary['clustering']['best_silhouette_score'] = best_score
            
            # PCAæ–¹å·®è§£é‡Šæ¯”ä¾‹
            if 'pca' in reduction_results.get('reduction_results', {}):
                pca_result = reduction_results['reduction_results']['pca']
                if 'cumulative_variance_ratio' in pca_result:
                    summary['dimensionality_reduction']['variance_explained'] = \
                        pca_result['cumulative_variance_ratio'][-1]
            
            return summary
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆåˆ†ææ‘˜è¦å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def save_results(self, results: dict, output_dir: str = "output"):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
            
            # ä¿å­˜ä¸»è¦ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœ
            import json
            results_file = output_path / f"ccgl_analysis_results_{timestamp}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            # ä¿å­˜ç®€åŒ–çš„æ±‡æ€»æŠ¥å‘Š
            summary_file = output_path / f"ccgl_analysis_summary_{timestamp}.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("CCGLä»“å‚¨ç®¡ç†ç³»ç»Ÿæ•°æ®åˆ†ææŠ¥å‘Š\\n")
                f.write("=" * 50 + "\\n")
                f.write(f"åˆ†ææ—¶é—´: {results.get('analysis_time', 'N/A')}\\n\\n")
                
                if 'summary' in results:
                    summary = results['summary']
                    f.write("æ•°æ®å¤„ç†æ‘˜è¦:\\n")
                    f.write(f"  - åŸå§‹æ•°æ®å½¢çŠ¶: {summary.get('data_processing', {}).get('original_shape')}\\n")
                    f.write(f"  - å¤„ç†åå½¢çŠ¶: {summary.get('data_processing', {}).get('final_shape')}\\n")
                    f.write(f"  - ç§»é™¤è¡Œæ•°: {summary.get('data_processing', {}).get('rows_removed')}\\n\\n")
                    
                    f.write("èšç±»åˆ†ææ‘˜è¦:\\n")
                    f.write(f"  - æœ€ä¼˜èšç±»æ•°: {summary.get('clustering', {}).get('optimal_clusters')}\\n")
                    f.write(f"  - æœ€ä½³æ–¹æ³•: {summary.get('clustering', {}).get('best_method')}\\n")
                    f.write(f"  - æœ€ä½³è½®å»“ç³»æ•°: {summary.get('clustering', {}).get('best_silhouette_score')}\\n\\n")
                    
                    f.write("å¼‚å¸¸æ£€æµ‹æ‘˜è¦:\\n")
                    f.write(f"  - å…±è¯†å¼‚å¸¸ç‚¹: {summary.get('anomalies', {}).get('consensus_anomalies')}\\n")
                    f.write(f"  - æ€»æ ·æœ¬æ•°: {summary.get('anomalies', {}).get('total_samples')}\\n\\n")
            
            self.logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            return str(results_file), str(summary_file)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            raise
    
    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        try:
            # 1. åˆå§‹åŒ–ç³»ç»Ÿ
            if not self.initialize_system():
                return False
            
            # 2. ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆåœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œè¿™é‡Œä¼šä»æ•°æ®åº“åŠ è½½æ•°æ®ï¼‰
            self.logger.info("å‡†å¤‡æ•°æ®...")
            sample_data = self.generate_sample_data()
            
            # 3. è¿è¡Œæ•°æ®è´¨é‡è¯„ä¼°
            self.logger.info("è¿è¡Œæ•°æ®è´¨é‡è¯„ä¼°...")
            quality_report = self.run_data_quality_assessment(sample_data)
            
            # 4. è¿è¡Œç»¼åˆåˆ†æ
            self.logger.info("è¿è¡Œç»¼åˆæ•°æ®åˆ†æ...")
            analysis_results = self.run_comprehensive_analysis(sample_data)
            
            # 5. åˆå¹¶ç»“æœ
            final_results = {
                'mode': 'basic_analysis',
                'data_quality_assessment': quality_report,
                'comprehensive_analysis': analysis_results
            }
            
            # 6. ä¿å­˜ç»“æœ
            self.logger.info("ä¿å­˜åˆ†æç»“æœ...")
            results_file, summary_file = self.save_results(final_results)
            
            # 7. æ˜¾ç¤ºå®Œæˆä¿¡æ¯
            self.logger.info("=" * 60)
            self.logger.info("ğŸ‰ CCGLä»“å‚¨ç®¡ç†ç³»ç»Ÿæ•°æ®åˆ†æå®Œæˆï¼")
            self.logger.info("=" * 60)
            self.logger.info(f"ğŸ“Š åˆ†ææ¨¡å¼: åŸºç¡€åˆ†ææ¨¡å¼")
            self.logger.info(f"ğŸ“ è¯¦ç»†ç»“æœ: {results_file}")
            self.logger.info(f"ğŸ“„ æ‘˜è¦æŠ¥å‘Š: {summary_file}")
            self.logger.info(f"ğŸ” æ•°æ®æ ·æœ¬: {sample_data.shape[0]} è¡Œ, {sample_data.shape[1]} åˆ—")
            
            if 'summary' in analysis_results:
                summary = analysis_results['summary']
                self.logger.info(f"ğŸ¯ æœ€ä¼˜èšç±»æ•°: {summary.get('clustering', {}).get('optimal_clusters')}")
                self.logger.info(f"ğŸš¨ å¼‚å¸¸ç‚¹æ•°é‡: {summary.get('anomalies', {}).get('consensus_anomalies')}")
            
            self.logger.info("\\nğŸ’¡ æç¤º:")
            self.logger.info("  - ä½¿ç”¨ python main_mcp.py å¯åŠ¨MCPæ¶æ„æ¨¡å¼")
            self.logger.info("  - ä½¿ç”¨ python main_llm.py å¯åŠ¨AIå¢å¼ºæ¨¡å¼")
            self.logger.info("  - ä½¿ç”¨ python quick_start.py å¿«é€Ÿæ¼”ç¤º")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
            return False
        
        finally:
            # æ¸…ç†èµ„æº
            if self.db_manager:
                self.db_manager.close()

def main():
    """ä¸»å…¥å£å‡½æ•°"""
    app = CCGLAnalyticsMain()
    success = app.run()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()