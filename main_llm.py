#!/usr/bin/env python3
"""
CCGL Analytics System - LLM Integration Main Program
AI-enhanced analysis mode with natural language querying
"""

import argparse
import asyncio
import sys
import os
import yaml
import json
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import time

# Add the project root to the Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from ccgl_analytics.utils.logger import get_logger, setup_logging

class LLMProvider:
    """Base class for LLM providers."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize LLM provider.
        
        Args:
            config: Provider configuration
        """
        self.config = config
        self.logger = get_logger(__name__)
    
    async def generate_response(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Generate response from LLM.
        
        Args:
            prompt: Input prompt
            context: Optional context information
            
        Returns:
            Generated response
        """
        raise NotImplementedError("Subclasses must implement generate_response")
    
    def is_available(self) -> bool:
        """Check if provider is available and configured.
        
        Returns:
            True if available
        """
        api_key = self.config.get('api_key')
        return bool(api_key and api_key.strip())

class OpenAIProvider(LLMProvider):
    """OpenAI GPT provider."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = config.get('model', 'gpt-4')
        self.max_tokens = config.get('max_tokens', 4000)
        self.temperature = config.get('temperature', 0.7)
    
    async def generate_response(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Generate response using OpenAI API."""
        if not self.is_available():
            return "OpenAI API key not configured."
        
        # Simulate OpenAI API call (in real implementation, use openai library)
        self.logger.info(f"Generating response using OpenAI {self.model}")
        await asyncio.sleep(1)  # Simulate API delay
        
        # Create contextual response
        response = f"""Based on your query about the data analysis:

{prompt}

Here's my analysis:

1. **Data Overview**: I understand you're working with a dataset that requires comprehensive analysis.

2. **Key Insights**: 
   - The data appears to have interesting patterns that warrant further investigation
   - There may be correlations between different variables that could provide business value
   - Anomalies in the data might indicate opportunities or issues that need attention

3. **Recommendations**:
   - Consider segmenting the data by key demographics or time periods
   - Apply clustering algorithms to identify natural groupings
   - Use anomaly detection to find outliers that might represent opportunities

4. **Next Steps**:
   - Perform detailed statistical analysis
   - Create visualizations to better understand patterns
   - Develop predictive models if appropriate

Would you like me to dive deeper into any specific aspect of this analysis?

*Generated by OpenAI {self.model}*"""
        
        return response

class ClaudeProvider(LLMProvider):
    """Anthropic Claude provider."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = config.get('model', 'claude-3-sonnet-20240229')
        self.max_tokens = config.get('max_tokens', 4000)
    
    async def generate_response(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Generate response using Claude API."""
        if not self.is_available():
            return "Anthropic Claude API key not configured."
        
        self.logger.info(f"Generating response using Claude {self.model}")
        await asyncio.sleep(1)  # Simulate API delay
        
        response = f"""I'll help you analyze this data thoughtfully and comprehensively.

Regarding your query: {prompt}

**Analytical Approach:**

Let me break down what I see and recommend:

üîç **Data Examination**
- First, I'd examine the data structure and quality
- Look for missing values, outliers, and data types
- Understand the business context and objectives

üìä **Statistical Analysis**
- Descriptive statistics to understand distributions
- Correlation analysis to identify relationships
- Hypothesis testing where appropriate

ü§ñ **Machine Learning Insights**
- Clustering to find natural segments
- Classification or regression based on objectives
- Feature importance and selection

üí° **Business Intelligence**
- Translate technical findings into actionable insights
- Identify opportunities for optimization
- Recommend specific next steps

**Questions for you:**
1. What specific business questions are you trying to answer?
2. What time period does this data cover?
3. Are there any known data quality issues?

I'm ready to dive deeper into any aspect you'd like to explore further.

*Analyzed by Claude {self.model}*"""
        
        return response

class ZhipuAIProvider(LLMProvider):
    """ZhipuAI GLM provider."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = config.get('model', 'glm-4')
    
    async def generate_response(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Generate response using ZhipuAI API."""
        if not self.is_available():
            return "ZhipuAI API key not configured."
        
        self.logger.info(f"Generating response using ZhipuAI {self.model}")
        await asyncio.sleep(1)  # Simulate API delay
        
        response = f"""ÊÇ®Â•ΩÔºÅÊàëÊù•‰∏∫ÊÇ®ÂàÜÊûêËøô‰∏™Êï∞ÊçÆÈóÆÈ¢ò„ÄÇ

ÂÖ≥‰∫éÊÇ®ÁöÑÊü•ËØ¢Ôºö{prompt}

**Êô∫ËÉΩÂàÜÊûêÊñπÊ°àÔºö**

üéØ **Êï∞ÊçÆÁêÜËß£**
- Êï∞ÊçÆÁª¥Â∫¶ÂíåËßÑÊ®°ÂàÜÊûê
- Êï∞ÊçÆË¥®ÈáèËØÑ‰º∞ÔºàÁº∫Â§±ÂÄº„ÄÅÂºÇÂ∏∏ÂÄº„ÄÅÈáçÂ§çÂÄºÔºâ
- ‰∏öÂä°ËÉåÊôØÂíåÂàÜÊûêÁõÆÊ†áÁ°ÆËÆ§

üìà **Ê∑±Â∫¶ÂàÜÊûê**
- ÊèèËø∞ÊÄßÁªüËÆ°ÂàÜÊûêÔºå‰∫ÜËß£Êï∞ÊçÆÂàÜÂ∏ÉÁâπÂæÅ
- Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÔºåÂèëÁé∞ÂèòÈáèÈó¥ÂÖ≥Á≥ª
- Ë∂ãÂäøÂàÜÊûêÔºåËØÜÂà´Êó∂Èó¥Â∫èÂàóÊ®°Âºè

üî¨ **Êú∫Âô®Â≠¶‰π†Ê¥ûÂØü**
- ËÅöÁ±ªÂàÜÊûêÔºåÂèëÁé∞Áî®Êà∑/‰∫ßÂìÅÂàÜÁæ§
- ÂºÇÂ∏∏Ê£ÄÊµãÔºåËØÜÂà´ÊΩúÂú®È£éÈô©ÊàñÊú∫‰ºö
- È¢ÑÊµãÂª∫Ê®°ÔºåÊîØÊåÅ‰∏öÂä°ÂÜ≥Á≠ñ

üíº **ÂïÜ‰∏ö‰ª∑ÂÄº**
- Â∞ÜÊäÄÊúØÂèëÁé∞ËΩ¨Âåñ‰∏∫‰∏öÂä°Ê¥ûÂØü
- Êèê‰æõÂèØÊâßË°åÁöÑË°åÂä®Âª∫ËÆÆ
- ÈáèÂåñÂàÜÊûêÁöÑÂïÜ‰∏öÂΩ±Âìç

**ÂêéÁª≠Âª∫ËÆÆÔºö**
1. Á°ÆÂÆöÊ†∏ÂøÉ‰∏öÂä°ÈóÆÈ¢òÂíåKPIÊåáÊ†á
2. Âª∫Á´ãÊï∞ÊçÆË¥®ÈáèÁõëÊéßÊú∫Âà∂
3. Âà∂ÂÆöÂü∫‰∫éÊï∞ÊçÆÁöÑÂÜ≥Á≠ñÊµÅÁ®ã

ÊàëÂèØ‰ª•ÈíàÂØπÊÇ®ÁöÑÂÖ∑‰ΩìÈúÄÊ±ÇÊèê‰æõÊõ¥ËØ¶ÁªÜÁöÑÂàÜÊûê„ÄÇ

*Áî±Êô∫Ë∞±AI {self.model}ÁîüÊàê*"""
        
        return response

class QwenProvider(LLMProvider):
    """Alibaba Tongyi Qianwen provider."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = config.get('model', 'qwen-turbo')
    
    async def generate_response(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Generate response using Qwen API."""
        if not self.is_available():
            return "Dashscope (Qwen) API key not configured."
        
        self.logger.info(f"Generating response using Qwen {self.model}")
        await asyncio.sleep(1)  # Simulate API delay
        
        response = f"""Âü∫‰∫éÊÇ®ÁöÑÊï∞ÊçÆÂàÜÊûêÈúÄÊ±ÇÔºåÊàëÊù•Êèê‰æõ‰∏ì‰∏öÁöÑÂàÜÊûêÂª∫ËÆÆ„ÄÇ

ÊÇ®ÁöÑÊü•ËØ¢Ôºö{prompt}

**Êï∞ÊçÆÂàÜÊûêÁ≠ñÁï•Ôºö**

üîç **Êï∞ÊçÆÊé¢Á¥¢Èò∂ÊÆµ**
- Êï∞ÊçÆÊ¶ÇËßàÔºöË°åÊï∞„ÄÅÂàóÊï∞„ÄÅÊï∞ÊçÆÁ±ªÂûãÂàÜÂ∏É
- Êï∞ÊçÆË¥®ÈáèËØÑ‰º∞ÔºöÂÆåÊï¥ÊÄß„ÄÅÂáÜÁ°ÆÊÄß„ÄÅ‰∏ÄËá¥ÊÄß
- ‰∏öÂä°ÁêÜËß£ÔºöÊòéÁ°ÆÂàÜÊûêÁõÆÊ†áÂíåÈ¢ÑÊúüÊàêÊûú

üìä **ÂàÜÊûêÂª∫Ê®°Èò∂ÊÆµ**
- ÁâπÂæÅÂ∑•Á®ãÔºöÊï∞ÊçÆÊ∏ÖÊ¥ó„ÄÅÂèòÊç¢„ÄÅË°çÁîü
- ÁªüËÆ°ÂàÜÊûêÔºöÊèèËø∞ÊÄßÁªüËÆ°„ÄÅÁõ∏ÂÖ≥ÂàÜÊûê„ÄÅÂàÜÂ∏ÉÊ£ÄÈ™å
- Êú∫Âô®Â≠¶‰π†ÔºöËÅöÁ±ª„ÄÅÂàÜÁ±ª„ÄÅÂõûÂΩí„ÄÅÂºÇÂ∏∏Ê£ÄÊµã

üéØ **Ê¥ûÂØüÊèêÂèñÈò∂ÊÆµ**
- Ê®°ÂºèËØÜÂà´ÔºöÂèëÁé∞Êï∞ÊçÆ‰∏≠ÁöÑËßÑÂæãÂíåË∂ãÂäø
- ÂºÇÂ∏∏ÂàÜÊûêÔºöËØÜÂà´ÂÄºÂæóÂÖ≥Ê≥®ÁöÑÂºÇÂ∏∏ÊÉÖÂÜµ
- ÂÖ≥ËÅîÂàÜÊûêÔºöÊåñÊéòÂèòÈáèÈó¥ÁöÑÊ∑±Â±ÇÂÖ≥Á≥ª

üí° **‰ª∑ÂÄºËΩ¨ÂåñÈò∂ÊÆµ**
- ‰∏öÂä°Ê¥ûÂØüÔºöÂ∞ÜÊäÄÊúØÁªìÊûúËΩ¨Âåñ‰∏∫‰∏öÂä°ËØ≠Ë®Ä
- Ë°åÂä®Âª∫ËÆÆÔºöÊèê‰æõÂÖ∑‰ΩìÂèØÊâßË°åÁöÑÊîπËøõÊñπÊ°à
- È£éÈô©ËØÑ‰º∞ÔºöËØÜÂà´ÊΩúÂú®È£éÈô©ÂíåÊú∫‰ºöÁÇπ

**ÂÆûÊñΩË∑ØÂæÑÔºö**
1. Âà∂ÂÆöËØ¶ÁªÜÁöÑÂàÜÊûêËÆ°ÂàíÂíåÊó∂Èó¥Ë°®
2. Âª∫Á´ãÊï∞ÊçÆË¥®ÈáèÊ†áÂáÜÂíåÁõëÊéß‰ΩìÁ≥ª
3. ËÆæËÆ°ÂèØÊåÅÁª≠ÁöÑÂàÜÊûêÂíåÊä•ÂëäÊú∫Âà∂

ËØ∑ÂëäËØâÊàëÊÇ®Â∏åÊúõÈáçÁÇπÂÖ≥Ê≥®Âì™‰∏™ÊñπÈù¢Ôºü

*ÈÄö‰πâÂçÉÈóÆ{self.model}Êèê‰æõ*"""
        
        return response

class LLMManager:
    """Manager for LLM providers."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize LLM manager.
        
        Args:
            config: LLM configuration
        """
        self.config = config
        self.logger = get_logger(__name__)
        self.providers = {}
        self._initialize_providers()
    
    def _initialize_providers(self):
        """Initialize all configured providers."""
        providers_config = self.config.get('providers', {})
        
        if 'openai' in providers_config:
            self.providers['openai'] = OpenAIProvider(providers_config['openai'])
        
        if 'claude' in providers_config:
            self.providers['claude'] = ClaudeProvider(providers_config['claude'])
        
        if 'zhipuai' in providers_config:
            self.providers['zhipuai'] = ZhipuAIProvider(providers_config['zhipuai'])
        
        if 'qwen' in providers_config:
            self.providers['qwen'] = QwenProvider(providers_config['qwen'])
        
        available_providers = [name for name, provider in self.providers.items() if provider.is_available()]
        self.logger.info(f"Initialized LLM providers: {available_providers}")
    
    def get_provider(self, provider_name: Optional[str] = None) -> LLMProvider:
        """Get LLM provider by name or default.
        
        Args:
            provider_name: Name of the provider, or None for default
            
        Returns:
            LLM provider instance
        """
        if provider_name is None:
            provider_name = self.config.get('default_provider', 'openai')
        
        if provider_name not in self.providers:
            raise ValueError(f"Provider '{provider_name}' not configured")
        
        provider = self.providers[provider_name]
        if not provider.is_available():
            raise ValueError(f"Provider '{provider_name}' not available (missing API key)")
        
        return provider
    
    def list_available_providers(self) -> List[str]:
        """List all available providers.
        
        Returns:
            List of available provider names
        """
        return [name for name, provider in self.providers.items() if provider.is_available()]

class NaturalLanguageQueryEngine:
    """Engine for processing natural language queries."""
    
    def __init__(self, llm_manager: LLMManager):
        """Initialize query engine.
        
        Args:
            llm_manager: LLM manager instance
        """
        self.llm_manager = llm_manager
        self.logger = get_logger(__name__)
        self.conversation_history = []
    
    async def process_query(self, query: str, data_context: Optional[Dict[str, Any]] = None, provider: Optional[str] = None) -> Dict[str, Any]:
        """Process natural language query.
        
        Args:
            query: Natural language query
            data_context: Optional data context
            provider: Optional specific provider to use
            
        Returns:
            Query processing results
        """
        self.logger.info(f"Processing natural language query: {query[:100]}...")
        
        # Get LLM provider
        llm_provider = self.llm_manager.get_provider(provider)
        
        # Build enhanced prompt with context
        enhanced_prompt = self._build_enhanced_prompt(query, data_context)
        
        # Generate response
        response = await llm_provider.generate_response(enhanced_prompt, data_context)
        
        # Store in conversation history
        self.conversation_history.append({
            'timestamp': pd.Timestamp.now().isoformat(),
            'query': query,
            'provider': provider or self.llm_manager.config.get('default_provider'),
            'response': response
        })
        
        return {
            'query': query,
            'response': response,
            'provider': provider or self.llm_manager.config.get('default_provider'),
            'data_context': data_context,
            'processing_time': time.time()
        }
    
    def _build_enhanced_prompt(self, query: str, data_context: Optional[Dict[str, Any]] = None) -> str:
        """Build enhanced prompt with context.
        
        Args:
            query: Original query
            data_context: Data context
            
        Returns:
            Enhanced prompt
        """
        prompt_parts = [
            "You are an expert data analyst helping with comprehensive data analysis.",
            "",
            "Context:"
        ]
        
        if data_context:
            if 'data_summary' in data_context:
                prompt_parts.append(f"Data Summary: {data_context['data_summary']}")
            
            if 'analysis_results' in data_context:
                prompt_parts.append(f"Previous Analysis: {data_context['analysis_results']}")
            
            if 'business_context' in data_context:
                prompt_parts.append(f"Business Context: {data_context['business_context']}")
        
        prompt_parts.extend([
            "",
            "User Query:",
            query,
            "",
            "Please provide a comprehensive, actionable response that includes:",
            "1. Analysis of the question/request",
            "2. Specific insights based on the data context",
            "3. Actionable recommendations",
            "4. Next steps or follow-up questions"
        ])
        
        return "\n".join(prompt_parts)
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Get conversation history.
        
        Returns:
            List of conversation entries
        """
        return self.conversation_history.copy()
    
    def clear_conversation_history(self):
        """Clear conversation history."""
        self.conversation_history.clear()
        self.logger.info("Conversation history cleared")

class InteractiveAnalysisSession:
    """Interactive analysis session with LLM integration."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize interactive session.
        
        Args:
            config: System configuration
        """
        self.config = config
        self.logger = get_logger(__name__)
        self.llm_manager = LLMManager(config.get('llm', {}))
        self.query_engine = NaturalLanguageQueryEngine(self.llm_manager)
        self.current_data = None
        self.current_data_summary = None
    
    async def start_session(self):
        """Start interactive analysis session."""
        self.logger.info("Starting interactive LLM analysis session")
        
        print("ü§ñ Welcome to CCGL Analytics System - AI Enhanced Mode")
        print("=" * 60)
        print("Available LLM providers:", ", ".join(self.llm_manager.list_available_providers()))
        print("Type 'help' for commands, 'exit' to quit")
        print("=" * 60)
        
        while True:
            try:
                user_input = input("\nüîç Query> ").strip()
                
                if user_input.lower() in ['exit', 'quit', 'q']:
                    print("üëã Goodbye!")
                    break
                
                elif user_input.lower() == 'help':
                    self._show_help()
                
                elif user_input.lower() == 'providers':
                    self._show_providers()
                
                elif user_input.lower() == 'history':
                    self._show_history()
                
                elif user_input.lower() == 'clear':
                    self.query_engine.clear_conversation_history()
                    print("üìù Conversation history cleared")
                
                elif user_input.lower().startswith('load '):
                    await self._load_data(user_input[5:].strip())
                
                elif user_input.lower().startswith('provider '):
                    await self._set_provider(user_input[9:].strip())
                
                elif user_input.lower() == 'data':
                    self._show_data_summary()
                
                elif user_input.startswith('!'):
                    # System command
                    await self._execute_system_command(user_input[1:].strip())
                
                elif user_input:
                    # Natural language query
                    await self._process_natural_language_query(user_input)
                
            except KeyboardInterrupt:
                print("\nüëã Session interrupted. Goodbye!")
                break
            except EOFError:
                print("\nüëã Session ended. Goodbye!")
                break
            except Exception as e:
                self.logger.error(f"Session error: {e}")
                print(f"‚ùå Error: {e}")
    
    def _show_help(self):
        """Show help information."""
        help_text = """
ü§ñ CCGL Analytics - AI Enhanced Mode Commands:

Data Management:
  load <file>        - Load data from file (CSV, Excel, JSON)
  data               - Show current data summary

Analysis:
  <natural query>    - Ask any question about your data
  provider <name>    - Switch LLM provider (openai, claude, zhipuai, qwen)
  
Session Management:
  providers          - List available LLM providers
  history            - Show conversation history
  clear              - Clear conversation history
  help               - Show this help
  exit/quit/q        - Exit session

System Commands (prefix with !):
  !status            - Show system status
  !config            - Show configuration
  
Examples:
  "What patterns do you see in the sales data?"
  "How can I improve customer retention?"
  "Are there any anomalies in the recent transactions?"
  "Recommend next steps for analysis"
        """
        print(help_text)
    
    def _show_providers(self):
        """Show available providers."""
        providers = self.llm_manager.list_available_providers()
        current_provider = self.llm_manager.config.get('default_provider', 'None')
        
        print(f"\nüîß Available LLM Providers:")
        for provider in providers:
            marker = "‚úÖ" if provider == current_provider else "  "
            print(f"  {marker} {provider}")
        
        print(f"\nCurrent default: {current_provider}")
    
    def _show_history(self):
        """Show conversation history."""
        history = self.query_engine.get_conversation_history()
        
        if not history:
            print("üìù No conversation history")
            return
        
        print(f"\nüìù Conversation History ({len(history)} entries):")
        print("-" * 50)
        
        for i, entry in enumerate(history[-5:], 1):  # Show last 5 entries
            timestamp = entry['timestamp']
            query = entry['query'][:80] + "..." if len(entry['query']) > 80 else entry['query']
            provider = entry['provider']
            
            print(f"{i}. [{timestamp}] ({provider})")
            print(f"   Q: {query}")
            print()
    
    async def _load_data(self, file_path: str):
        """Load data from file.
        
        Args:
            file_path: Path to data file
        """
        try:
            if not os.path.exists(file_path):
                print(f"‚ùå File not found: {file_path}")
                return
            
            # Load data based on file extension
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext == '.csv':
                self.current_data = pd.read_csv(file_path)
            elif file_ext in ['.xlsx', '.xls']:
                self.current_data = pd.read_excel(file_path)
            elif file_ext == '.json':
                self.current_data = pd.read_json(file_path)
            else:
                print(f"‚ùå Unsupported file format: {file_ext}")
                return
            
            # Generate data summary
            self.current_data_summary = {
                'shape': self.current_data.shape,
                'columns': list(self.current_data.columns),
                'dtypes': self.current_data.dtypes.to_dict(),
                'missing_values': self.current_data.isnull().sum().to_dict(),
                'numeric_columns': list(self.current_data.select_dtypes(include=['number']).columns)
            }
            
            print(f"‚úÖ Data loaded successfully from {file_path}")
            print(f"   Shape: {self.current_data.shape[0]} rows, {self.current_data.shape[1]} columns")
            
        except Exception as e:
            self.logger.error(f"Failed to load data: {e}")
            print(f"‚ùå Failed to load data: {e}")
    
    async def _set_provider(self, provider_name: str):
        """Set LLM provider.
        
        Args:
            provider_name: Name of the provider
        """
        available_providers = self.llm_manager.list_available_providers()
        
        if provider_name not in available_providers:
            print(f"‚ùå Provider '{provider_name}' not available")
            print(f"Available providers: {', '.join(available_providers)}")
            return
        
        self.llm_manager.config['default_provider'] = provider_name
        print(f"‚úÖ Switched to provider: {provider_name}")
    
    def _show_data_summary(self):
        """Show current data summary."""
        if self.current_data is None:
            print("üìä No data loaded. Use 'load <file>' to load data.")
            return
        
        print(f"\nüìä Current Data Summary:")
        print(f"  Shape: {self.current_data_summary['shape']}")
        print(f"  Columns: {len(self.current_data_summary['columns'])}")
        print(f"  Numeric columns: {len(self.current_data_summary['numeric_columns'])}")
        
        missing_count = sum(v for v in self.current_data_summary['missing_values'].values() if v > 0)
        if missing_count > 0:
            print(f"  Missing values: {missing_count} total")
    
    async def _execute_system_command(self, command: str):
        """Execute system command.
        
        Args:
            command: System command
        """
        if command == 'status':
            print("üîß System Status:")
            print(f"  LLM Providers: {len(self.llm_manager.list_available_providers())}")
            print(f"  Data Loaded: {'Yes' if self.current_data is not None else 'No'}")
            print(f"  Conversation Entries: {len(self.query_engine.get_conversation_history())}")
        
        elif command == 'config':
            print("‚öôÔ∏è Configuration Summary:")
            print(f"  Default Provider: {self.llm_manager.config.get('default_provider')}")
            print(f"  Available Providers: {', '.join(self.llm_manager.list_available_providers())}")
        
        else:
            print(f"‚ùå Unknown system command: {command}")
    
    async def _process_natural_language_query(self, query: str):
        """Process natural language query.
        
        Args:
            query: Natural language query
        """
        print("ü§î Processing your question...")
        
        # Build data context
        data_context = {}
        if self.current_data_summary:
            data_context['data_summary'] = self.current_data_summary
        
        try:
            result = await self.query_engine.process_query(query, data_context)
            
            print(f"\nü§ñ Response from {result['provider']}:")
            print("-" * 50)
            print(result['response'])
            
        except Exception as e:
            self.logger.error(f"Query processing failed: {e}")
            print(f"‚ùå Failed to process query: {e}")

def main():
    """Main entry point for LLM mode."""
    parser = argparse.ArgumentParser(
        description="CCGL Analytics System - LLM Integration Mode",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --interactive
  %(prog)s --query "What patterns are in the sales data?" --data sales.csv
  %(prog)s --provider claude --interactive
        """
    )
    
    parser.add_argument(
        '-c', '--config',
        default='config.yml',
        help='Configuration file path (default: config.yml)'
    )
    
    parser.add_argument(
        '--interactive', '-i',
        action='store_true',
        help='Start interactive session'
    )
    
    parser.add_argument(
        '--query', '-q',
        help='Single natural language query'
    )
    
    parser.add_argument(
        '--data',
        help='Data file for context'
    )
    
    parser.add_argument(
        '--provider', '-p',
        help='LLM provider to use (openai, claude, zhipuai, qwen)'
    )
    
    parser.add_argument(
        '--output', '-o',
        help='Output file for results'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Enable verbose logging'
    )
    
    args = parser.parse_args()
    
    # Load configuration
    try:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"Configuration file not found: {args.config}")
        sys.exit(1)
    except yaml.YAMLError as e:
        print(f"Error parsing configuration file: {e}")
        sys.exit(1)
    
    # Setup logging
    logging_config = config.get('logging', {})
    setup_logging(
        level='DEBUG' if args.verbose else logging_config.get('level', 'INFO'),
        format_type=logging_config.get('format', 'text'),
        log_file=logging_config.get('file'),
    )
    
    logger = get_logger(__name__)
    logger.info("Starting CCGL Analytics System - LLM Mode")
    
    async def main_async():
        if args.interactive:
            # Interactive mode
            session = InteractiveAnalysisSession(config)
            await session.start_session()
        
        elif args.query:
            # Single query mode
            llm_manager = LLMManager(config.get('llm', {}))
            query_engine = NaturalLanguageQueryEngine(llm_manager)
            
            # Load data if provided
            data_context = {}
            if args.data:
                try:
                    if args.data.endswith('.csv'):
                        data = pd.read_csv(args.data)
                    elif args.data.endswith(('.xlsx', '.xls')):
                        data = pd.read_excel(args.data)
                    elif args.data.endswith('.json'):
                        data = pd.read_json(args.data)
                    else:
                        print(f"Unsupported file format: {args.data}")
                        return 1
                    
                    data_context['data_summary'] = {
                        'shape': data.shape,
                        'columns': list(data.columns),
                        'dtypes': data.dtypes.to_dict()
                    }
                    
                except Exception as e:
                    print(f"Failed to load data: {e}")
                    return 1
            
            # Process query
            try:
                result = await query_engine.process_query(
                    args.query, 
                    data_context, 
                    args.provider
                )
                
                print(f"Query: {result['query']}")
                print(f"Provider: {result['provider']}")
                print("-" * 50)
                print(result['response'])
                
                # Save to file if specified
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, indent=2, ensure_ascii=False)
                    print(f"\nResults saved to: {args.output}")
                
            except Exception as e:
                logger.error(f"Query processing failed: {e}")
                print(f"Error: {e}")
                return 1
        
        else:
            print("Error: Either --interactive or --query must be specified")
            return 1
        
        return 0
    
    # Run main async function
    try:
        return asyncio.run(main_async())
    except KeyboardInterrupt:
        logger.info("LLM system interrupted by user")
        return 130

if __name__ == "__main__":
    sys.exit(main())