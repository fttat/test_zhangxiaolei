#!/usr/bin/env python3
"""
CCGLä»“å‚¨ç®¡ç†ç³»ç»Ÿæ•°æ®åˆ†æå·¥ç¨‹ - AIå¢å¼ºæ¨¡å¼

é›†æˆå¤§è¯­è¨€æ¨¡å‹ï¼Œæä¾›è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œæ™ºèƒ½ä¸šåŠ¡æ´å¯ŸåŠŸèƒ½ã€‚
"""

import os
import sys
import pandas as pd
import webbrowser
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from ccgl_analytics.modules.data_connection import DataConnectionManager
from ccgl_analytics.modules.data_preprocessing import DataPreprocessor  
from ccgl_analytics.modules.analysis_core import AnalysisCore
from ccgl_analytics.modules.llm_config_manager import LLMConfigManager
from ccgl_analytics.modules.web_dashboard import WebDashboard
from ccgl_analytics.utils.config_loader import ConfigLoader
from ccgl_analytics.utils.logger_setup import setup_logger

class CCGLAnalyticsLLM:
    """CCGLåˆ†æç³»ç»ŸAIå¢å¼ºç‰ˆ"""
    
    def __init__(self):
        """åˆå§‹åŒ–AIå¢å¼ºç³»ç»Ÿ"""
        self.logger = setup_logger("ccgl_llm")
        self.config_loader = ConfigLoader()
        self.db_manager = None
        self.preprocessor = None
        self.analyzer = None
        self.llm_manager = None
        self.dashboard = None
        
    def initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        try:
            self.logger.info("=== CCGLä»“å‚¨ç®¡ç†ç³»ç»Ÿæ•°æ®åˆ†æå·¥ç¨‹å¯åŠ¨ ===")
            self.logger.info("æ¨¡å¼: AIå¢å¼ºåˆ†ææ¨¡å¼ ğŸ¤–")
            
            # åŠ è½½é…ç½®
            config = self.config_loader.load_config()
            self.logger.info("é…ç½®åŠ è½½å®Œæˆ")
            
            # åˆå§‹åŒ–åŸºç¡€ç»„ä»¶
            db_config = config.get('database', {})
            self.db_manager = DataConnectionManager(db_config)
            self.logger.info("æ•°æ®åº“è¿æ¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            preprocessing_config = config.get('preprocessing', {})
            self.preprocessor = DataPreprocessor(preprocessing_config)
            self.logger.info("æ•°æ®é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            analysis_config = config.get('analysis', {})
            self.analyzer = AnalysisCore(analysis_config)
            self.logger.info("åˆ†ææ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–AIç»„ä»¶
            llm_config = config.get('llm', {})
            self.llm_manager = LLMConfigManager(llm_config)
            self.logger.info("LLMé…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–Webä»ªè¡¨æ¿
            web_config = config.get('web', {})
            self.dashboard = WebDashboard(web_config)
            self.logger.info("Webä»ªè¡¨æ¿åˆå§‹åŒ–å®Œæˆ")
            
            # æ˜¾ç¤ºAIèƒ½åŠ›çŠ¶æ€
            self._show_ai_capabilities()
            
            self.logger.info("AIå¢å¼ºç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ âœ¨")
            return True
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _show_ai_capabilities(self):
        """æ˜¾ç¤ºAIèƒ½åŠ›çŠ¶æ€"""
        config_summary = self.llm_manager.get_config_summary()
        
        self.logger.info("ğŸ¤– AIèƒ½åŠ›çŠ¶æ€:")
        self.logger.info(f"  - å¯ç”¨AIæä¾›å•†: {', '.join(config_summary['available_providers']) if config_summary['available_providers'] else 'æ—  (å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼)'}")
        self.logger.info(f"  - é»˜è®¤æ¨¡å‹: {config_summary['default_model']}")
        self.logger.info(f"  - OpenAIå¯ç”¨: {'âœ…' if config_summary['openai_available'] else 'âŒ'}")
        self.logger.info(f"  - Anthropicå¯ç”¨: {'âœ…' if config_summary['anthropic_available'] else 'âŒ'}")
        
        if not config_summary['available_providers']:
            self.logger.warning("ğŸ’¡ æç¤º: é…ç½®AI APIå¯†é’¥ä»¥è·å¾—çœŸæ­£çš„AIå¢å¼ºåŠŸèƒ½")
    
    def generate_sample_data(self) -> pd.DataFrame:
        """ç”Ÿæˆç¤ºä¾‹ä»“å‚¨æ•°æ®"""
        try:
            import numpy as np
            
            # ç”Ÿæˆæ›´çœŸå®çš„ä»“å‚¨ç®¡ç†æ•°æ®
            n_samples = 1200
            np.random.seed(42)
            
            # ä»“åº“å’Œäº§å“æ•°æ®
            warehouses = ['åŒ—äº¬æ™ºèƒ½ä»“å‚¨ä¸­å¿ƒ', 'ä¸Šæµ·ç‰©æµæ¢çº½', 'å¹¿å·é…é€ä¸­å¿ƒ', 'æ·±åœ³ç”µå•†ä»“åº“']
            products = ['æ™ºèƒ½æ‰‹æœº', 'ç¬”è®°æœ¬ç”µè„‘', 'å¹³æ¿ç”µè„‘', 'æ™ºèƒ½æ‰‹è¡¨', 'æ— çº¿è€³æœº', 'å……ç”µå®']
            suppliers = ['åä¸ºæŠ€æœ¯', 'å°ç±³ç§‘æŠ€', 'è‹¹æœå…¬å¸', 'ä¸‰æ˜Ÿç”µå­', 'è”æƒ³é›†å›¢']
            
            # ç”Ÿæˆç›¸å…³æ€§æ›´å¼ºçš„æ•°æ®
            base_prices = {'æ™ºèƒ½æ‰‹æœº': 3000, 'ç¬”è®°æœ¬ç”µè„‘': 6000, 'å¹³æ¿ç”µè„‘': 2500, 
                          'æ™ºèƒ½æ‰‹è¡¨': 1500, 'æ— çº¿è€³æœº': 800, 'å……ç”µå®': 200}
            
            data = {
                'warehouse_id': np.random.choice(warehouses, n_samples),
                'product_id': np.random.choice(products, n_samples),
                'supplier_id': np.random.choice(suppliers, n_samples),
                'quantity': np.random.randint(50, 2000, n_samples),
                'unit_price': [],
                'storage_cost': np.random.uniform(5, 100, n_samples),
                'order_date': pd.date_range(start='2024-01-01', periods=n_samples, freq='6H'),
                'delivery_days': np.random.randint(1, 20, n_samples),
                'quality_score': np.random.uniform(0.7, 1.0, n_samples),
                'temperature': np.random.uniform(18, 26, n_samples),
                'humidity': np.random.uniform(35, 75, n_samples),
                'customer_satisfaction': np.random.uniform(3.0, 5.0, n_samples)
            }
            
            # åŸºäºäº§å“ç±»å‹ç”Ÿæˆä»·æ ¼
            for product in data['product_id']:
                base_price = base_prices.get(product, 1000)
                variation = np.random.uniform(0.8, 1.3)
                data['unit_price'].append(round(base_price * variation, 2))
            
            df = pd.DataFrame(data)
            
            # æ·»åŠ ä¸šåŠ¡é€»è¾‘å…³è”
            # é«˜ä»·å€¼äº§å“éœ€è¦æ›´å¥½çš„å­˜å‚¨æ¡ä»¶
            high_value_mask = df['unit_price'] > 3000
            df.loc[high_value_mask, 'storage_cost'] *= 1.5
            df.loc[high_value_mask, 'quality_score'] += 0.1
            df.loc[high_value_mask, 'quality_score'] = df.loc[high_value_mask, 'quality_score'].clip(0, 1)
            
            # æ·»åŠ å­£èŠ‚æ€§å› ç´ 
            df['month'] = df['order_date'].dt.month
            holiday_months = [11, 12, 1, 2]  # å‡æ—¥è´­ç‰©å­£
            holiday_mask = df['month'].isin(holiday_months)
            df.loc[holiday_mask, 'quantity'] *= 1.3
            df.loc[holiday_mask, 'unit_price'] *= 1.1
            
            # å¼•å…¥ä¸€äº›ç°å®çš„ç¼ºå¤±å€¼å’Œå¼‚å¸¸
            missing_indices = np.random.choice(df.index, size=int(0.03 * len(df)), replace=False)
            df.loc[missing_indices[:len(missing_indices)//2], 'quality_score'] = np.nan
            df.loc[missing_indices[len(missing_indices)//2:], 'customer_satisfaction'] = np.nan
            
            # å¼•å…¥åˆç†çš„å¼‚å¸¸å€¼ï¼ˆä¾‹å¦‚ä¿ƒé”€æ´»åŠ¨å¯¼è‡´çš„å¤§è®¢å•ï¼‰
            promotion_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)
            df.loc[promotion_indices, 'quantity'] *= 5
            df.loc[promotion_indices, 'unit_price'] *= 0.7  # ä¿ƒé”€é™ä»·
            
            self.logger.info(f"ç”ŸæˆAIå¢å¼ºç¤ºä¾‹æ•°æ®: {df.shape}")
            return df
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç¤ºä¾‹æ•°æ®å¤±è´¥: {e}")
            raise
    
    def run_enhanced_analysis(self, df: pd.DataFrame) -> dict:
        """è¿è¡ŒAIå¢å¼ºçš„ç»¼åˆåˆ†æ"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹AIå¢å¼ºæ•°æ®åˆ†æ")
            
            # 1. åŸºç¡€æ•°æ®åˆ†æï¼ˆä¸main.pyç›¸åŒï¼‰
            self.logger.info("æ­¥éª¤1: åŸºç¡€æ•°æ®é¢„å¤„ç†")
            cleaned_data, preprocessing_report = self.preprocessor.clean_data(
                df, 
                missing_strategy='auto',
                outlier_method='iqr',
                scaling_method='standard'
            )
            
            self.logger.info("æ­¥éª¤2: æœºå™¨å­¦ä¹ åˆ†æ")
            clustering_results = self.analyzer.clustering_analysis(
                cleaned_data,
                methods=['kmeans', 'dbscan', 'hierarchical']
            )
            
            anomaly_results = self.analyzer.anomaly_detection(
                cleaned_data,
                methods=['isolation_forest', 'one_class_svm']
            )
            
            reduction_results = self.analyzer.dimensionality_reduction(
                cleaned_data,
                methods=['pca', 'tsne'],
                n_components=2
            )
            
            enhanced_data, feature_report = self.preprocessor.create_features(cleaned_data)
            
            # 2. AIå¢å¼ºä¸šåŠ¡æ´å¯Ÿç”Ÿæˆ
            self.logger.info("æ­¥éª¤3: ğŸ¤– AIä¸šåŠ¡æ´å¯Ÿç”Ÿæˆ")
            
            # æ„å»ºåˆ†æç»“æœ
            base_analysis = {
                'data_preprocessing': preprocessing_report,
                'feature_engineering': feature_report,
                'clustering_analysis': clustering_results,
                'anomaly_detection': anomaly_results,
                'dimensionality_reduction': reduction_results
            }
            
            # ä½¿ç”¨AIç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ
            ai_insights = self.llm_manager.generate_business_insights(base_analysis)
            
            # 3. æ™ºèƒ½æ•°æ®æŸ¥è¯¢æ¼”ç¤º
            self.logger.info("æ­¥éª¤4: ğŸ” æ™ºèƒ½æ•°æ®æŸ¥è¯¢æ¼”ç¤º")
            data_context = self._prepare_data_context(df, base_analysis)
            
            # æ¼”ç¤ºå‡ ä¸ªè‡ªç„¶è¯­è¨€æŸ¥è¯¢
            demo_queries = [
                "åˆ†ææˆ‘ä»¬çš„åº“å­˜æ•°æ®ä¸­æœ€éœ€è¦å…³æ³¨çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ",
                "å“ªä¸ªä»“åº“çš„è¿è¥æ•ˆç‡æœ€é«˜ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ",
                "å¼‚å¸¸æ•°æ®å¯èƒ½è¡¨æ˜ä»€ä¹ˆä¸šåŠ¡é—®é¢˜ï¼Ÿ",
                "å¦‚ä½•ä¼˜åŒ–æˆ‘ä»¬çš„åº“å­˜ç®¡ç†ç­–ç•¥ï¼Ÿ"
            ]
            
            query_results = []
            for query in demo_queries:
                result = self.llm_manager.natural_language_query(query, data_context)
                query_results.append(result)
                self.logger.info(f"  âœ“ å®ŒæˆæŸ¥è¯¢: {query[:30]}...")
            
            # 4. ç”ŸæˆAIå¢å¼ºä»ªè¡¨æ¿
            self.logger.info("æ­¥éª¤5: ğŸ“Š ç”ŸæˆAIå¢å¼ºä»ªè¡¨æ¿")
            enhanced_analysis = {
                **base_analysis,
                'ai_insights': ai_insights,
                'natural_language_queries': query_results
            }
            
            dashboard_html = self.dashboard.create_fastapi_dashboard({
                'data_quality_assessment': self._create_quality_assessment(df),
                'comprehensive_analysis': enhanced_analysis
            })
            
            dashboard_file = self.dashboard.save_dashboard(
                dashboard_html, 
                f"ccgl_ai_dashboard_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            )
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            final_results = {
                'mode': 'ai_enhanced_analysis',
                'analysis_time': datetime.now().isoformat(),
                'data_preprocessing': preprocessing_report,
                'machine_learning_analysis': {
                    'clustering': clustering_results,
                    'anomaly_detection': anomaly_results,
                    'dimensionality_reduction': reduction_results
                },
                'feature_engineering': feature_report,
                'ai_insights': ai_insights,
                'natural_language_queries': query_results,
                'dashboard_file': dashboard_file,
                'summary': self._generate_ai_summary(
                    preprocessing_report, clustering_results, anomaly_results, ai_insights
                )
            }
            
            self.logger.info("AIå¢å¼ºæ•°æ®åˆ†æå®Œæˆ ğŸ‰")
            return final_results
            
        except Exception as e:
            self.logger.error(f"AIå¢å¼ºåˆ†æå¤±è´¥: {e}")
            raise
    
    def _prepare_data_context(self, df: pd.DataFrame, analysis_results: dict) -> str:
        """å‡†å¤‡æ•°æ®ä¸Šä¸‹æ–‡ç”¨äºAIæŸ¥è¯¢"""
        context = f"""
        ä»“å‚¨ç®¡ç†æ•°æ®åˆ†æä¸Šä¸‹æ–‡ï¼š
        
        ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š
        - æ€»è®°å½•æ•°: {len(df)} æ¡
        - æ•°æ®ç»´åº¦: {len(df.columns)} åˆ—
        - ä»“åº“æ•°é‡: {df['warehouse_id'].nunique()}
        - äº§å“ç§ç±»: {df['product_id'].nunique()}
        - ä¾›åº”å•†æ•°é‡: {df['supplier_id'].nunique()}
        - æ—¶é—´èŒƒå›´: {df['order_date'].min()} åˆ° {df['order_date'].max()}
        
        ğŸ“ˆ å…³é”®æŒ‡æ ‡ç»Ÿè®¡ï¼š
        - å¹³å‡åº“å­˜æ•°é‡: {df['quantity'].mean():.0f}
        - å¹³å‡å•ä»·: {df['unit_price'].mean():.2f}å…ƒ
        - å¹³å‡å­˜å‚¨æˆæœ¬: {df['storage_cost'].mean():.2f}å…ƒ
        - å¹³å‡è´¨é‡è¯„åˆ†: {df['quality_score'].mean():.2f}
        - å¹³å‡é…é€å¤©æ•°: {df['delivery_days'].mean():.1f}å¤©
        - å¹³å‡å®¢æˆ·æ»¡æ„åº¦: {df['customer_satisfaction'].mean():.2f}/5.0
        
        ğŸ¯ æœºå™¨å­¦ä¹ åˆ†æç»“æœï¼š
        - æœ€ä¼˜èšç±»æ•°é‡: {analysis_results.get('clustering_analysis', {}).get('optimal_clusters', 'N/A')}
        - æ£€æµ‹åˆ°çš„å¼‚å¸¸ç‚¹: {analysis_results.get('anomaly_detection', {}).get('summary', {}).get('consensus_anomalies', 0)} ä¸ª
        
        ğŸ·ï¸ ä¸»è¦ä¸šåŠ¡ç‰¹å¾ï¼š
        - ä¸»è¦ä»“åº“: {', '.join(df['warehouse_id'].value_counts().head(3).index.tolist())}
        - çƒ­é—¨äº§å“: {', '.join(df['product_id'].value_counts().head(3).index.tolist())}
        - ä¸»è¦ä¾›åº”å•†: {', '.join(df['supplier_id'].value_counts().head(3).index.tolist())}
        
        ğŸ’° è´¢åŠ¡æŒ‡æ ‡ï¼š
        - æ€»åº“å­˜ä»·å€¼: {(df['quantity'] * df['unit_price']).sum():,.0f}å…ƒ
        - æ€»å­˜å‚¨æˆæœ¬: {(df['quantity'] * df['storage_cost']).sum():,.0f}å…ƒ
        - å¹³å‡åº“å­˜å‘¨è½¬: åŸºäº{df['delivery_days'].mean():.1f}å¤©é…é€å‘¨æœŸ
        """
        
        return context
    
    def _create_quality_assessment(self, df: pd.DataFrame) -> dict:
        """åˆ›å»ºæ•°æ®è´¨é‡è¯„ä¼°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return {
            'table_name': 'ai_enhanced_warehouse_data',
            'assessment_time': datetime.now().isoformat(),
            'basic_stats': {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'numeric_columns': df.select_dtypes(include=['number']).shape[1],
                'categorical_columns': df.select_dtypes(include=['object']).shape[1],
                'datetime_columns': df.select_dtypes(include=['datetime64']).shape[1]
            },
            'data_quality_metrics': {
                'completeness': (df.shape[0] * df.shape[1] - df.isnull().sum().sum()) / (df.shape[0] * df.shape[1]),
                'consistency': 0.96,
                'validity': 0.97,
                'uniqueness': len(df.drop_duplicates()) / len(df)
            },
            'missing_values': {
                'total_missing': int(df.isnull().sum().sum()),
                'missing_by_column': df.isnull().sum().to_dict()
            }
        }
    
    def _generate_ai_summary(self, preprocessing_report, clustering_results, 
                           anomaly_results, ai_insights) -> dict:
        """ç”ŸæˆAIå¢å¼ºçš„åˆ†ææ‘˜è¦"""
        summary = {
            'data_processing': {
                'original_shape': preprocessing_report.get('original_shape'),
                'final_shape': preprocessing_report.get('final_shape'),
                'rows_removed': preprocessing_report.get('removed_rows', 0)
            },
            'machine_learning': {
                'optimal_clusters': clustering_results.get('optimal_clusters'),
                'anomalies_detected': anomaly_results.get('summary', {}).get('consensus_anomalies', 0)
            },
            'ai_enhancement': {
                'insights_generated': ai_insights.get('success', False),
                'ai_model_used': ai_insights.get('insights', {}).get('response', '').startswith('ğŸ“ˆ') if ai_insights.get('success') else False,
                'business_recommendations': True if ai_insights.get('success') else False
            }
        }
        
        return summary
    
    def save_ai_results(self, results: dict, output_dir: str = "output"):
        """ä¿å­˜AIå¢å¼ºåˆ†æç»“æœ"""
        try:
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜å®Œæ•´ç»“æœ
            import json
            results_file = output_path / f"ccgl_ai_analysis_results_{timestamp}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            # ä¿å­˜AIæ´å¯ŸæŠ¥å‘Š
            if results.get('ai_insights', {}).get('success'):
                insights_file = output_path / f"ccgl_ai_insights_{timestamp}.txt"
                with open(insights_file, 'w', encoding='utf-8') as f:
                    f.write("CCGLä»“å‚¨ç®¡ç†ç³»ç»Ÿ - AIå¢å¼ºåˆ†ææ´å¯ŸæŠ¥å‘Š\\n")
                    f.write("=" * 60 + "\\n")
                    f.write(f"ç”Ÿæˆæ—¶é—´: {results.get('analysis_time')}\\n\\n")
                    
                    ai_response = results['ai_insights'].get('insights', {}).get('response', '')
                    f.write("ğŸ¤– AIä¸šåŠ¡æ´å¯Ÿ:\\n")
                    f.write(ai_response)
                    f.write("\\n\\n")
                    
                    f.write("ğŸ” è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç»“æœ:\\n")
                    for i, query_result in enumerate(results.get('natural_language_queries', []), 1):
                        if query_result.get('success'):
                            f.write(f"{i}. æŸ¥è¯¢: {query_result.get('query')}\\n")
                            f.write(f"   å›ç­”: {query_result.get('response', '')[:200]}...\\n\\n")
            
            # ä¿å­˜ç®€åŒ–æ‘˜è¦
            summary_file = output_path / f"ccgl_ai_summary_{timestamp}.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("CCGLä»“å‚¨ç®¡ç†ç³»ç»Ÿ - AIå¢å¼ºåˆ†ææ‘˜è¦\\n")
                f.write("=" * 50 + "\\n")
                f.write(f"åˆ†ææ—¶é—´: {results.get('analysis_time')}\\n\\n")
                
                summary = results.get('summary', {})
                f.write("æ•°æ®å¤„ç†æ‘˜è¦:\\n")
                f.write(f"  - åŸå§‹æ•°æ®: {summary.get('data_processing', {}).get('original_shape')}\\n")
                f.write(f"  - å¤„ç†å: {summary.get('data_processing', {}).get('final_shape')}\\n")
                f.write(f"  - èšç±»æ•°é‡: {summary.get('machine_learning', {}).get('optimal_clusters')}\\n")
                f.write(f"  - å¼‚å¸¸ç‚¹: {summary.get('machine_learning', {}).get('anomalies_detected')}\\n")
                f.write(f"  - AIæ´å¯Ÿ: {'âœ…' if summary.get('ai_enhancement', {}).get('insights_generated') else 'âŒ'}\\n")
                f.write(f"  - ä»ªè¡¨æ¿: {results.get('dashboard_file', 'N/A')}\\n")
            
            self.logger.info(f"AIåˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            return str(results_file), str(summary_file)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜AIç»“æœå¤±è´¥: {e}")
            raise
    
    def interactive_ai_query(self):
        """äº¤äº’å¼AIæŸ¥è¯¢ç•Œé¢"""
        self.logger.info("\\nğŸ¤– è¿›å…¥äº¤äº’å¼AIæŸ¥è¯¢æ¨¡å¼")
        self.logger.info("è¾“å…¥'quit'æˆ–'exit'é€€å‡º")
        
        # å‡†å¤‡ä¸€äº›ç¤ºä¾‹æ•°æ®ä¸Šä¸‹æ–‡
        sample_context = """
        å½“å‰ä»“å‚¨ç³»ç»ŸåŒ…å«4ä¸ªä¸»è¦ä»“åº“ï¼Œ6ç§äº§å“ç±»åˆ«ï¼Œ5ä¸ªä¾›åº”å•†ã€‚
        å¹³å‡åº“å­˜é‡çº¦1000ä»¶ï¼Œå•ä»·èŒƒå›´500-8000å…ƒã€‚
        æ£€æµ‹åˆ°4ä¸ªä¸»è¦çš„äº§å“ç¾¤ä½“å’Œçº¦15ä¸ªå¼‚å¸¸æ•°æ®ç‚¹ã€‚
        """
        
        while True:
            try:
                user_query = input("\\nğŸ” è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢ (æˆ–è¾“å…¥'quit'é€€å‡º): ").strip()
                
                if user_query.lower() in ['quit', 'exit', 'é€€å‡º']:
                    self.logger.info("é€€å‡ºäº¤äº’å¼æŸ¥è¯¢æ¨¡å¼")
                    break
                
                if not user_query:
                    continue
                
                self.logger.info(f"ğŸ¤– AIæ­£åœ¨å¤„ç†æ‚¨çš„æŸ¥è¯¢...")
                result = self.llm_manager.natural_language_query(user_query, sample_context)
                
                if result.get('success'):
                    print("\\n" + "="*60)
                    print("ğŸ¤– AIå›ç­”:")
                    print("="*60)
                    print(result.get('response', ''))
                    print("="*60)
                    print(f"ä½¿ç”¨æ¨¡å‹: {result.get('model_used', 'unknown')}")
                else:
                    print(f"\\nâŒ æŸ¥è¯¢å¤±è´¥: {result.get('error', 'Unknown error')}")
                
            except KeyboardInterrupt:
                self.logger.info("\\nç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºäº¤äº’æ¨¡å¼")
                break
            except Exception as e:
                self.logger.error(f"æŸ¥è¯¢è¿‡ç¨‹å‡ºé”™: {e}")
    
    def run(self):
        """è¿è¡ŒAIå¢å¼ºä¸»ç¨‹åº"""
        try:
            # 1. åˆå§‹åŒ–ç³»ç»Ÿ
            if not self.initialize_system():
                return False
            
            # 2. ç”Ÿæˆç¤ºä¾‹æ•°æ®
            self.logger.info("å‡†å¤‡AIå¢å¼ºæ•°æ®...")
            sample_data = self.generate_sample_data()
            
            # 3. è¿è¡ŒAIå¢å¼ºåˆ†æ
            self.logger.info("è¿è¡ŒAIå¢å¼ºç»¼åˆåˆ†æ...")
            analysis_results = self.run_enhanced_analysis(sample_data)
            
            # 4. ä¿å­˜ç»“æœ
            self.logger.info("ä¿å­˜AIåˆ†æç»“æœ...")
            results_file, summary_file = self.save_ai_results(analysis_results)
            
            # 5. æ˜¾ç¤ºå®Œæˆä¿¡æ¯
            self.logger.info("=" * 70)
            self.logger.info("ğŸ‰ CCGLä»“å‚¨ç®¡ç†ç³»ç»ŸAIå¢å¼ºåˆ†æå®Œæˆï¼")
            self.logger.info("=" * 70)
            self.logger.info(f"ğŸ¤– åˆ†ææ¨¡å¼: AIå¢å¼ºåˆ†ææ¨¡å¼")
            self.logger.info(f"ğŸ“ è¯¦ç»†ç»“æœ: {results_file}")
            self.logger.info(f"ğŸ“„ æ‘˜è¦æŠ¥å‘Š: {summary_file}")
            self.logger.info(f"ğŸ“Š AIä»ªè¡¨æ¿: {analysis_results.get('dashboard_file', 'N/A')}")
            self.logger.info(f"ğŸ” æ•°æ®æ ·æœ¬: {sample_data.shape[0]} è¡Œ, {sample_data.shape[1]} åˆ—")
            
            # æ˜¾ç¤ºAIæ´å¯Ÿé¢„è§ˆ
            if analysis_results.get('ai_insights', {}).get('success'):
                ai_response = analysis_results['ai_insights']['insights'].get('response', '')
                preview = ai_response[:200] + "..." if len(ai_response) > 200 else ai_response
                self.logger.info(f"ğŸ§  AIæ´å¯Ÿé¢„è§ˆ: {preview}")
            
            # è¯¢é—®æ˜¯å¦æ‰“å¼€ä»ªè¡¨æ¿
            dashboard_file = analysis_results.get('dashboard_file')
            if dashboard_file and Path(dashboard_file).exists():
                try:
                    response = input("\\nğŸŒ æ˜¯å¦åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€AIä»ªè¡¨æ¿? (y/N): ").strip().lower()
                    if response in ['y', 'yes', 'æ˜¯']:
                        webbrowser.open(f"file://{Path(dashboard_file).absolute()}")
                        self.logger.info("ğŸ“Š ä»ªè¡¨æ¿å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€")
                except:
                    pass
            
            # è¯¢é—®æ˜¯å¦è¿›å…¥äº¤äº’å¼AIæŸ¥è¯¢
            try:
                response = input("\\nğŸ’¬ æ˜¯å¦è¿›å…¥äº¤äº’å¼AIæŸ¥è¯¢æ¨¡å¼? (y/N): ").strip().lower()
                if response in ['y', 'yes', 'æ˜¯']:
                    self.interactive_ai_query()
            except:
                pass
            
            self.logger.info("\\nğŸ’¡ æç¤º:")
            self.logger.info("  - ä½¿ç”¨ python main.py è¿è¡ŒåŸºç¡€åˆ†ææ¨¡å¼")
            self.logger.info("  - ä½¿ç”¨ python main_mcp.py è¿è¡ŒMCPæ¶æ„æ¨¡å¼")
            self.logger.info("  - é…ç½®AI APIå¯†é’¥ä»¥è·å¾—æ›´å¼ºå¤§çš„åˆ†æèƒ½åŠ›")
            
            return True
            
        except Exception as e:
            self.logger.error(f"AIå¢å¼ºç¨‹åºè¿è¡Œå¤±è´¥: {e}")
            return False
        
        finally:
            # æ¸…ç†èµ„æº
            if self.db_manager:
                self.db_manager.close()

def main():
    """ä¸»å…¥å£å‡½æ•°"""
    app = CCGLAnalyticsLLM()
    success = app.run()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()